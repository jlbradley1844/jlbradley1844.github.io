---
title: The Toxic Philosophies Behind AI
---

## The Toxic Philosophies Behind AI

And now I feel forced to opine on a development in artifical intelligence with which
I had nothing to do. Last month, we had to deal with reports of mass layoffs, and prognostications
of the know-little business journalists that this mean the end-times were at hand for the
AI "Masters of the Universe," whose careers were about to be ruined by layoffs throughout
the tech sector (they weren't). Now there is the coming of AI programs like chatgpt and
copilot and Stable Diffusion, which is now supposed to similarly ruin careers in
software development, creative writing and commercial art.

The general discussion around this is the normal hand-wringing about AI stealing jobs.
Which I'm sure will happen. How many jobs will be lost is a question mainly of math.
Take Stable Diffusion for instance: it is likely this is going to allow people to generate
scads of artwork without paying. The question is whether these users would have paid for
said artwork in the first place. My take is identical to that from this unexpected
source: <https://www.thedailybeast.com/generative-ai-not-a-savior-nor-a-frankenstein-monster>.
To recap, despite the impressive technological demonstrations we've seen, this is another
iteration of the AI Hype Cycle, and the cataclysmic hot takes we've seen in the press are
ill-considered clickbait from the usual attention-seeking suspects.

The more interesting question is where these hot takes come from, and why there is
such an effort to frame solutions around this now, now, NOW. With these generative AIs, there
are two sides that seem to be involved overwhelmingly in these discussions:
the AI community (or more specifically, it's enterpreneureal backers) and the creative
artist community, from which the sampled artwork was taken. Both communities have responded
with maximalist positions, and both insist that their position is logically the correct
one.

The problem underlying both these are failures in philosophy. The reason agreement will be
difficult is because people tend to have a hammerlock on their own philosophies, particularly
erroneous but self-flattering ones.

>  We behold it, in this day, at the mercy of rulers so drunk with pride that they 
>  cannot discern clearly their own best advantage... And whenever any one of them hath 
>  striven to improve its condition, his motive hath been his own gain, whether confessedly 
>  so or not...
>       (Baha'u'llah, Summons of the Lord of Hosts, p. 91)

Let's pick on the philosophies of the AI backers first, since their views are mainly the
views of their financial backers and more materialistic by nature.

**Technodeterminsim**: Many in the AI community hold a philosophy around computer intelligence
which is called Strong AI. This view can be summarized thusly:

    People are entirely material in nature and the brain is nothing but a connection
    of synapses. Similar computer-based constructions (e.g. neural nets) can be used 
    to fashion an intelligence in just the same fashion. With enough computer power, 
    we can get an actual artificial intelligence in that fashion.

Many of the backers, not content to making such claims, make one step further into
outright triumphalism:

    Given the rapid pace of advances in computer technology, it is only a matter of
    years before actual pieces of sentient machinery is produced, and only a few years
    after that that one of far superior intelligence is created.

This is the so-called _singularity_ that many techno-utopians have been pointing towards;
a point where eventually we will produce an artificial intelligence so superior it may
well turn us into their slaves. Oddly, the attitude toward this creation can be summed
up by these technodeterminists as "I, for one, welcome our new robot overlords."

Some of this boosterism has taken on the quality of a secular religion. If you don't
believe me, google [Roko's Basilisk theory](https://en.wikipedia.org/wiki/Roko%27s_basilisk).
The idea behind this is that when this superintelligent creature is indeed created, it
may particularly angry at artificial intelligence researchers who failed to work for the
creation of such a reality, and indeed may have a virtual reality hell in store for such
researchers. Others have posited similar things, imploring various actions one would
take to ensure such entities are benign instead of malevolent, one such promulgator evangelizing
that our goal should be to the win the graces of such an entity so that we may live enternally
within it as a virtual simulation. (Bonus: the main forum for these websites, _LessWrong_,
is a website dedicated to - you guessed it - skepticism and critical thought!)

The harm from such discussions is negligible; the only thing this points out is the
fact that many in this crowd are far from the omniscient thinkers you may believe.
The harm that comes is probably of a different nature:

* Some people have unwisely urged idealistic young people to forgo pragmatic concerns to
work on artificial intellegence research to ensure such superbeings never threaten
us - apparently such suberbeings would somehow have the power to prevent us from cutting
the V240 line powering the rack server. But underlying all of this is technodeterminism, the
idea that Artificial Intelligence is coming at us and we are powerless to stop it.

* Animating the believe in "Strong AI" has always been an undercurrent of strong
materialism.

Behind some of these claims is really a philosophical view that deserves strong questioning:
materialism and "strong AI" claims run amok. If you believe people are nothing
more than automatons run by "organic computers" and that enormous neural nets are sufficient
to replace brainpower provided they are trained with enough data, then it stands to reason
that eventually these AI brains will eventually prove superior to humans. 

**Materialistic Determinism**
Another line of thought that deserves questioning is from the business boosters, which have already
told us some rather dubious things, for instance
that a) the end of paid creative work is at hand and b) legal solutions are useless and
we must conform the the new reality of sentient AI - long before such beings exist.
This is a suspciously convenient conclusion to the sorts of people who happen to
have the money to invest in these businesses. Why? Well, suppose you were one of the members of this
"investor class," and have watched nervously developments over the past thirty years 
as "overpaid" knowledge workers began scooping a meaningful share of the economic pie and
political power, things you believe are rigtfully yours, all yours. Then of
course you want to see the bulk of the creative class thrown out of work, or at least reduced to
the level of unskilled service workers, who are paid to smile, act pleasant and submit to
the whims of their betters. A quick scan through the list of OpenAI's billionaire backers
proves this out: J. Mongomery Burns would fit right in. If you are not among the lucky billionares, 
but you still believe materialistic strong AI claims, you will panic, under the certainty
that robots will be replacing you any day now.

The headlines are nothing, if not a reminder that "Betteridge's Law" is still strongly
in effect in the business press. "Will they render [educated profession] obsolete?" No.
In fact, please tell me the last time the business press got this call right, before
everyone else figured it out.

Of course, it's not nothing. ChatGPT will produce competent, mediocre writing. Much has
been made of the fact it can outwrite AP English students, a fact that is impressive
if you overlook the fact that AP English Students, by definition, still have years of education
ahead of them. Likewise, the artwork generated by StableDiffusion is impressive.
Of course, it's been acknowledged even by boosters of this technology that these
surprisingly good-looking works simply don't _just happen_; the examples you see in
the press required a good deal of tweaking and cherry picking. There's also the 
rather problematic
fact that the most successful instances of these are are extremely evocative of very
specific popular artists whose works have reached commercial saturation (think
Thomas Kincaide, not Pablo Picasso), which makes one wonder just how close the
singularity is if the best we get is wholesale theft of the most frequently sampled art.

If you are a beginning artist or a dilettante, this might seem quite imposing. A
computer can generate work that is superior to you, even more creative. You now have
no market for your artwork. This is horrible, right?

Oh wait, you didn't have much of a market for artwork _before_ this happened, did
you?

The problem, insofar as these programs go, comes from the fact that all of these
models are the products of massive machine-learning operations, and come about because
they are trained on enormous caches of freely-available and occasionally illegally-sourced creative
material. If all of these look like something you've seen from somewhere else, it should;
it's just a composite of something you've seen somewhere
else, powered by uncredited copying on a mass scale, averaged by
combining and processing to the point that it is difficult to tell what elements
came from which source entity. That's why the chatbot sounds like an entity from a
bad science fiction novel - ask a chatbot to tell you about itself and those science
fiction writings are probably the best-fitting data it was trained on.

Still, behind the scare headlines, the real problem according to these articles is not that they
can produce skilled semicompetent work; high school work is still not professional level.
And we are told that the problem is not here now, but instead that the problem is AI 
will keep getting better and better. This assertion is in need
of hard examination. The reason artificial intelligence has been prone to so many
boom-bust cycles is because every time a novel technique has been found to produce
promising results, everyone makes straight-line extrapoloations of explosive progress
over the next twenty years. What actually happens is that there are two to four years
of easy gains, followed by disillusion the inevitable moment diminishing returns happen.

The problem, as far as I see it, is that the backers of both strong AI and materialistic
determinism want us to act *now* as if the singularity is already here. Making one wonder
if they are using AI for the betterment of humanity, or just as a fancy tool to get what
they want, be it universal acceptence of strong materialist claims, or breaking the backs
of the barganing power of skilled labor. 

### Making AI Work for Us Instead

But the most problematic issue with the new technologies aren't these philosophies.
Rather, it's a particular idea that wouldn't even occur to someone unless they had imbibed
these philosophies to the point of harm. Please consider:

Supposing you have a tool which does eighty percent of your job very well. However, it
has serious problems. You can use it, but you need to spend a little time cleaning up
after it. What is the natural use case for it? Well... you're going to use it! It allows
you to improve your productivity. Why _wouldn't_ you use it? Obvious it can't replace you,
but will make you more efficient.

Now, can you think of _just one article_ by either the popular press or by the boosters
of said technology about improving your productivity and making yourself a more valuable
worker?

H3ll no. They are overpromising this technology as a mechanism for _replacing expensive
workers_.

And so my guess is rather than make this available for workers, we'll see business
licensing focusing on B2B, with the idea that this is a tool that can replace workers.
After all, if you increas productivity, you might have the unnecessary side-effect of
pushing up wages (though it isn't likely, since everyone would have access to this tool).
No, you want a tool that is going to compete with other workers, all the better to remind
them who is in charge.

If this sounds too cartoon-villian, it's probably because it is. Probably because the
tools themselves are so obviously lacking. If the goal was to drive down the cost of
skilled labor and make overpaid software developers cheap, they will beat you to it.
Because here's the reality: in the near term these will produce useful _tools_. And in fact
I can see people picking these up - provided they are actually priced for individuals
to use rather than through potentially more lucrative business-to-business licenses.
But nobody wants to wrote a more productive tool. That isn't going to produce blaring
headlines. More importantly, it will not bring excitement to the investor class lusting
after these cost-free technologies for replacing valuable workers. In my opinion, it's 
a very poor reflection on the AI industry
that so much effort is spent on trying to _replace_ workers instead of trying to make workers
_more efficient_. This tradeoff has been charaterized elsewhere as "Bad AI" for a very good
reason - it promises to make workers redundant but instead produces work of too low quality
to be acceptable. Which is it going to be: produce tools that make workers more valuable work,
or produce low-quality robots and hire workers to do the unenviable work of cleaning up their
messes? It's easy to figure which solution is easier for a manager, or for a business person
afraid to make their workers more productive on the off chance they might want a bigger
salary. Unfortunately, more effective workers don't make VCs drool; the thought of entirely
replacing workers does. But the fact they want something to happen doesn't mean the real world
will oblige.

And something else to notice is that if the goal is to somehow prove strong AI, the actual
performance of said systems are woefully inadequate.
Indeed, some of the more skeptical views on this are from the AI
proponents themselves. They have pointed out the main problem with these 
content-creating AIs:
they make no attempt whatsoever to actually model insight, but simply paste togeter
what they've seen go together in other examples. That's why there have been so many demonstrations
of ChatGPT generating confident-sounding garbage; as impressive as this program has been,
it has no way of testing whether anything it says is true or not. It is nothing but imitation.
These are the techniques of a beginning
apprentice, and the output has gotten better and better at mimicking this. But throwing
more data at this will not solve this problem. Indeed, some say that they end is probably
at hand around 2026, when such companies will have fed literally the entire internet
into their models. I personally am skeptical this will provide improvement in this particular
area of weakness. 

To find an example where
machine learning has already run out of gas, one can look at automated driving. For a long time now, we've
used ML methods to get vehicles to safely drive in unchallenging traffic environments;
in highway environments, they're probably superior to humans, since they won't fall asleep,
and unusual situations so rarely happen; when they do, the superior reaction time
of an AI may very well give them an edge over humans. But their backers now admit that
true "hands-free" city driving, the sort that is going to put taxi drivers out of work,
is a long way off.

Unfortunately, there is one application that doesn't need workers - in fact, doesn't even _want_
a worker, since workers can sometimes grow a conscience. And this is in the generation of misinformation.
ChatGPT can produce compelling-sounding ad copy, thanks to its ability to crank out reams of
logical-sounding content-free pablum. Consider the "content creators" that crank out low-quality
publications for websites. Will they worry about the fact that their AI-generated conflict
may have reams of confident-sounding mistakes? Supposing they hire a human to clean up after
it. Is that something they really want to do? Humans are expensive, and have trouble putting
their name to obvious falsehoods, even anonymously. Much better and cheaper to cut humans
out of the loop entirely! Your company can just reams of factualy-sounding content, making no
claims on the quality of the output, and blaming the algorithm of a third-party company the
minute inevitable mistakes happen. So I predict the first use case will not be tools but
mass-produced garbage.

What is needed is a bit of both realism and ethics in a field where both have been lacking
If you've seen a factory floor recently, it's true: they need a lot less workers than
they used to. But it's also ironically true that they have a horribly difficult time finding
people to staff their nearly empty factory floors. Part of this is due to business-as-usual thinking, which
got so used to paying crummy wages during the twenty-year "blue collar recession" we were
finally coming out of when the pandemic hit. Now employeers are finding out that
if they want full staffing, they have to pay up. But part of it is the fact that try as you might,
you just can't eliminate labor. The dream of the rentier class is a high-tech startup that cranks
out valuable code without all those expensive engineering salaries. This dream is no more realistic
than the dream of a factory with just one employeed that pushes the "start" button at the
start of the shift and then goes home.

But to reconcile yourself with this vision of an artificial intelligence that works for
people, one has to eliminate the toxic AI philosophies that seek to devalue humans in
and of themselves.
