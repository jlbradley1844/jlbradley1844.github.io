---
title: The Toxic Philosophies Behind AI
---

## The Toxic Philosophies Behind AI

And now I feel forced to opine on a development in artifical intelligence with which
I had nothing to do. Last month, we had to deal with reports of mass layoffs, and prognostications
of the know-little business journalists that this mean the end-times were at hand for the
AI "Masters of the Universe," whose careers were about to be ruined by layoffs throughout
the tech sector (they weren't). Now there is the coming of AI programs like chatgpt and
copilot and Stable Diffusion, which is now supposed to similarly ruin careers in
software development, creative writing and commercial art.

The general discussion around this is the normal hand-wringing about AI stealing jobs.
Which I'm sure will happen. How many jobs will be lost is a question mainly of math.
Take Stable Diffusion for instance: it is likely this is going to allow people to generate
scads of artwork without paying. The question is whether these users would have paid for
said artwork in the first place. My take is identical to that from this unexpected
source: <https://www.thedailybeast.com/generative-ai-not-a-savior-nor-a-frankenstein-monster>.
To recap, despite the impressive technological demonstrations we've seen, this is another
iteration of the AI Hype Cycle, and the cataclysmic hot takes we've seen in the press are
ill-considered clickbait from the usual attention-seeking suspects.

The more interesting question is where these hot takes come from, and why there is
such an effort to frame solutions around this now, now, NOW. With these generative AIs, there
are two sides that seem to be involved overwhelmingly in these discussions:
the AI community (or more specifically, it's enterpreneureal backers) and the creative
artist community, from which the sampled artwork was taken. Both communities have responded
with maximalist positions, and both insist that their position is logically the correct
one.

The problem underlying both these are failures in philosophy. The reason agreement will be
difficult is because people tend to have a hammerlock on their own philosophies, particularly
erroneous but self-flattering ones.

>  We behold it, in this day, at the mercy of rulers so drunk with pride that they 
>  cannot discern clearly their own best advantage... And whenever any one of them hath 
>  striven to improve its condition, his motive hath been his own gain, whether confessedly 
>  so or not...
>       (Baha'u'llah, Summons of the Lord of Hosts, p. 91)

Let's pick on the philosophies of the AI backers first, since their views are mainly the
views of their financial backers and more materialistic by nature.

**Technodeterminsim**: Many in the AI community hold a philosophy around computer intelligence
which is called Strong AI. This view can be summarized thusly:

    People are entirely material in nature and the brain is nothing but a connection
    of synapses. Similar computer-based constructions (e.g. neural nets) can be used 
    to fashion an intelligence in just the same fashion. With enough computer power, 
    we can get an actual artificial intelligence in that fashion.

Many of the backers, not content to making such claims, make one step further into
outright triumphalism:

    Given the rapid pace of advances in computer technology, it is only a matter of
    years before actual pieces of sentient machinery is produced, and only a few years
    after that that one of far superior intelligence is created.

This is the so-called _singularity_ that many techno-utopians have been pointing towards;
a point where eventually we will produce an artificial intelligence so superior it may
well turn us into their slaves. Oddly, the attitude toward this creation can be summed
up by these technodeterminists as "I, for one, welcome our new robot overlords."

Some of this boosterism has taken on the quality of a secular religion. If you don't
believe me, google the [Roko's Basilisk theory](https://en.wikipedia.org/wiki/Roko%27s_basilisk).
The idea behind this is that when this superintelligent creature is indeed created, it
may particularly angry at artificial intelligence researchers who failed to work for the
creation of such a reality, and indeed may have a virtual reality hell in store for such
researchers. Others have posited similar things, imploring various actions one would
take to ensure such entities are benign instead of malevolent, one such promulgator evangelizing
that our goal should be to the win the graces of such an entity so that we may live enternally
within it as a virtual simulation. (Bonus: the main forum for these websites, _LessWrong_,
is a website dedicated to - you guessed it - skepticism and critical thought!)

The harm from such discussions is negligible; the only thing this points out is the
fact that many in this crowd are far from the omniscient thinkers you may believe.
The harm that comes is probably of a different nature:

* Some people have unwisely urged idealistic young people to forgo pragmatic concerns to
work on artificial intellegence research to ensure such superbeings never threaten
us - apparently such suberbeings would somehow have the power to prevent us from cutting
the V240 line powering the rack server. But underlying all of this is technodeterminism, the
idea that Artificial Intelligence is coming at us and we are powerless to stop it.

* Animating the believe in "Strong AI" has always been an undercurrent of strong
materialism.


But people under the spell of these lines of thought are precisely those people who are telling us
that a) the end of paid creative work is at hand and b) legal solutions are useless and
we must conform the the new reality of sentient AI - long before such beings exist.



The headlines are nothing, if not a reminder that "Betteridge's Law" is still strongly
in effect in the business press. "Will they render [educated profession] obsolete?" No.
In fact, please tell me the last time the business press got this call right, before
everyone else figured it out.

Of course, it's not nothing. ChatGPT will produce competent, mediocre writing. Much has
been made of the fact it can outwrite AP English students, a fact that is impressive
if you overlook the fact that AP English Students, by definition, still have years of education
ahead of them. Likewise, the artwork generated by StableDiffusion is impressive.
Of course, it's been acknowledged even by boosters of this technology that these
surprisingly good-looking works simply don't _just happen_; the examples you see in
the press required a good deal of tweaking and cherry picking. This isn't really a
problem of the program, as such; it's so little trouble to do multiple runs that it
doesn't keep you from getting the effect you want. There's also the rather problematic
fact that the most successful instances of these are are extremely evocative of very
specific popular artists whose works have reached commercial saturation. Think
Thomas Kincaide, not Pablo Picasso. And certainly not an inventive combination of the two.

If you are a beginning artist or a dilettante, this might seem quite imposing. A
computer can generate work that is superior to you, even more creative. You now have
no market for your artwork. This is horrible, right?

Oh wait, you didn't have much of a market for artwork _before_ this happened, did
you?

The problem, insofar as these programs go, comes from the fact that all of these
models are the products of massive machine-learning operations, and come about because
they are trained on enormous caches of bulk, possibly illegally-sourced creative
data. If all of these look like something you've seen from somewhere else, it should
seem obvious why; it's nothing more than a composite of something you've seen somewhere
else. It's powered by uncredited plaigarization on a mass scale, averaged by
combining and processing to the point that it is difficult to tell what elements
came from which source entity.

Still, this is a big problem for the beginning artist. How do you learn? You copy from
the masters. You learn how to paint like a Rembrandt, or in one of dozens of other
styles. You are, to some extent, a plaigarist, just like these ML counterparts. You
are skilled, but not creative. And if you want to sell commercial art that looks like
"generic commercial art," the market for this is even more unhealthy than it has
ever been. Now, the effective market for your works are now zero. The solution, such
as it is, is to be original enough as to be unimintable, hardly new advice and something
most artists take years to do. It's beyond the ken of the
beginning technician or skilled amateur practitioner. And so it seems the world of
less creative artistry is threatened.

Perhaps more troublesome are those established artists that have a defined look and
style and for which StableDiffision has no problem copying. Given that these programs
were trained on these artists images with no permission and using nothing other than
"fair use" guidelines, and fair use guidelines explicitly prohibit use of original works
in a manner that competes directly against copyright holders, I suspect that in the
court of law the established artists have the upper hand. The owners of the AI programs
have countered that their programs themselves do not infringe, and that it is the _user_
who may be infringing by generating artwork to imitative of a particular style, but the
companies may regret this stance.  Do they really want to tell the businesses they sell to
that the use of their image generators may produce infringing artwork that their own users will
be legally held responsible for?

Let's move on to the chatbots. Stack Overflow has shut down the use of chatbot-generated
submissions. The submissions may be gramatically correct, but too often they had too many critical
flaws, and in many cases they came with *shudder* "example code" that, more often
than not, was egregiously wrong. Where have these chatbots been successful, you might ask? Well,
if you want a mealy-mouthed essay encapsulating conventional points of view, this chatbot
is perfect. Hence, the response posted in reply to the Stack Overflow post announcing the
shutdown, a chatbot-written analysis of arguments for and against chatbot postings that
bring to mind the writings of content-free editorial essays that seem to
be aimed at making readers feel informed without actually telling them something novel.
Perhaps if these chatbots can put out of work the paid liars
who write anondyne op-eds as a means of disguising the true positions of industry titans
and politicians, they would have done a great service. But don't expect a chatbot to
come up with any unexpected insights. Machine learning engines are great big
averaging machines, and they are going to tell you what "everyone" thinks, not necessarily
what a trained expert thinks.

One last example I should find concerning is Co-Pilot,
an early beta-tested prototype of a chatbot paid to produce
working code. "Will copilot put programmers out of work?" No (again,
Betteridge's rule). It's been shown to reduce boilerplate. As a working programmer
I will tell you boilerplate is tedious and unenjoyable. I will also tell you that _it's actually
a very small part of the job_. If your job consists of 50% boilerplate writing, you truly have
done a poor job as a programmer. Your code has low information content, and you are working
at too low a level of abstraction. Things like this is why frameworks have been invented, and
you can expect tools like copilot to fill the gap only when you have programmers too unskilled
to work in an efficient manner, buy using proper libraries, DSLs, frameworks, and other
techniques that help you work at the level of the problem you are trying to solve.
If you don't do that, copilot is not your biggest threat; an impatient coworker who could
easily automate you out of your job is.

Behind the scare headlines, the real problem according to these articles is not that they
can produce skilled semicompetent work; high school work is still not professional level.
No, the problem is that AI will keep getting better and better. But that is not necessarily
what is going to happen. Indeed, some of the more skeptical views on this are from the AI
proponents themselves. The main problem with these content-creating AIs are the fact that
they make no attempt whatsoever to actually model insight; rather, they simply paste togeter
what they've seen go together in other examples. These are the techniques of a beginning
apprentice, and the output has gotten better and better at mimicking this. But throwing
more data at this will not solve this problem. Perhaps the best example of an area where
machine learning has run out of gas is in automated driving. For a long time now, we've
used ML methods to get vehicles to safely drive in unchallenging traffic environments;
in highway environments, they're probably superior to humans, since they won't fall asleep,
and unusual situations so rarely happen and when they do, the superior reaction time
of an AI may very well give them an edge over humans. But their backers now admit that
true "hands-free" city driving, the sort that is going to put taxi drivers out of work,
is a long way off.

Behind some of these claims is really a philosophical view that deserves strong questioning:
materialism and "strong AI" claims run amok. If you believe people are nothing
more than automatons run by "organic computers" and that enormous neural nets are sufficient
to replace brainpower provided they are trained with enough data, then it stands to reason
that eventually these AI brains will eventually prove superior to humans. If you hold that belief,
and if you are a member of the "investor class" that has watched jealously as knowledge workers
scoop up a meaningful share of the economic pie that you believe is rigtfully yours, all yours, then of
course you want to see the bulk of the creative class thrown out of work, or at least reduced to
the level of unskilled service workers, who are paid to smile, act pleasant and submit to
the whims of their betters. A quick scan through the list of OpenAI's billionaire backers
proves this out: J. Mongomery Burns would fit right in. If you are not among this group, 
but you still believe materialistic strong AI claims, you will panic.

Everyone else can worry a little bit less. But that doesn't mean we shouldn't still worry.
Why? Because I think the main problem is on the consumer side, not the supply side.
That is: if you are not a critical or knowledgeable consumer of this, then for all effective
purposes you will not be able to tell the difference. There are a few things that can happen
in response. One is that more skilled employees will be retained, since you will need skilled
people to curate the output and assure it is of sufficient quality, making the labor saving
much less than originally promised. Or you can just blast out the output via a subscription-based
service and make no claims on the quality of the output. Given the fact that most people can't
pass up a bargain, no matter how bad in the long term, I think the initial successes are going
to go to those willing to pump out garbage. That will prove popular in the short term - and probably
cause the field to lose face over the longer term.

I think eventully these will produce useful _tools_. But that fact isn't going to produce blaring
headlines - and perhaps more importantly, will not bring excitement to the investor class lusting
after these cost-free technologies. In my opinion, it's a very poor reflection on the AI industry
that so much effort is spent on trying to _replace_ workers instead of trying to make workers
_more efficient_. This tradeoff has been charaterized elsewhere as "Bad AI" for a very good
reason - it promises to make workers redundant but instead produces work of too low quality
to be acceptable. Which is it going to be: produce tools that make workers more valuable work,
or produce low-quality robots and hire workers to do the unenviable work of cleaning up their
messes? It's easy to figure which solution is easier for a manager, or for a business person
afraid to make their workers more productive on the off chance they might want a bigger
salary. Unfortunately, more effective workers don't make VCs drool; the thought of entirely
replacing workers does. But the fact they want something to happen doesn't mean the real world
will oblige.

If you've seen a factory floor recently, it's true: they need a lot less workers than
they used to. But it's also ironically true that they have a horribly difficult time finding
people to staff their nearly empty factory floors. Part of this is due to business-as-usual thinking, which
got so used to paying crummy wages during the twenty-year "blue collar recession" we were
finally coming out of when the pandemic hit. Now employeers are finding out that
if they want full staffing, they have to pay up. But part of it is the fact that try as you might,
you just can't eliminate labor. The dream of the rentier class is a high-tech startup that cranks
out valuable code without all those expensive engineering salaries. This dream is no more realistic
than the dream of a factory with just one employeed that pushes the "start" button at the
start of the shift and then goes home.

