---
title: The Toxic Philosophies Behind AI
---

## The Toxic Philosophies Behind AI
And now I feel forced to opine on a development in artifical intelligence with which
I had nothing to do. Amid continuing reports of technology layoffs,
attention-hungry business journalists are flogging claims that end-times are at hand 
for the creative-class "Masters of the Universe." AI programs like chatgpt and
copilot and Stable Diffusion are going to ruin careers in
software development, creative writing and commercial art. Best to run out and get a
job in, I dunno, something the robots cannot take, maybe real estate.

People read these articles because, obviously, they fear for their jobs.
While it seems an obvious danger, it may be untrue in practice.
Take Stable Diffusion for instance: it is likely this is going to allow people to generate
scads of artwork without paying. The question is whether these users would have paid for
said artwork in the first place. My take is identical to that from this pop news
source: <https://www.thedailybeast.com/generative-ai-not-a-savior-nor-a-frankenstein-monster>.
To this cynical observer, this looks like another
iteration of the AI Hype Cycle, and the cataclysmic hot takes we've seen in the press are
just clickbait from the usual attention-seeking suspects.

Since controversy sells, it's no surprise the press is eager to give voice to those
who want solutions now, now, NOW. Two sides seem especially invested here:
the enterprenereur backers of these AI technologies and creative
artists. Both communities have responded
with broadly maximalist positions, insisting on either complete license or heavy regulation.

I don't have a strong opinion on this, other than both claims are so extreme that
I'm sure they're both wrong. For instance, I don't fully agree with the creative classes
that their livelihood is under threat in spite of the fact that as a programmer, I are one. People will
continue to pay for non-derivative art, and if you know anything about these new AI
programs, its work is, by nature, _entirely derivative_. The availability of widely-available
clip-art and license-free music samples didn't kill them twenty years ago, and neither will this.
Still, they do have a point; no one is being compensated for these clearly-derivative products.

I have more problems with the AI "maximalists." My main problem is with certain philosophical
problems in the AI field. It isn't surprising to hear the business class clamoring about the evils of red tape,
regulation, and following all those Little People laws. But many of their backers have
compounded this by tons of bad philosophy. Indeed, if you look under the hood of some of
the philosophies driving AI development, you'll see much that is disturbing. These hardly seem 
like people who can be trusted to self-regulate their behavior.

>  We behold it, in this day, at the mercy of rulers so drunk with pride that they 
>  cannot discern clearly their own best advantage... And whenever any one of them hath 
>  striven to improve its condition, his motive hath been his own gain, whether confessedly 
>  so or not...
>       (Baha'u'llah, Summons of the Lord of Hosts, p. 91)

I'd like to detail some of the philosophical failures that make a consensus on the
proper use and place of AI so difficult.

**Technodeterminsim**: Many in the AI community hold a philosophy around computer intelligence
which is called _Strong AI_. This view can be summarized thusly:

>  People are entirely material in nature and the brain is nothing but a connection
>  of synapses. Similar computer-based constructions (e.g. neural nets) can be used 
>  to fashion an intelligence in just the same fashion. With enough computer power, 
>  we can get an actual artificial intelligence in that fashion.

Many true believers, not content with such "modest" claims, step even further into
outright triumphalism:

>  Given the rapid pace of advances in computer technology, it is only a matter of
>  years before actual pieces of sentient machinery is produced, and only a few years
>  after that that one of far superior intelligence is created.

This is the so-called _singularity_ that many techno-utopians have been pointing towards;
a point where mankind produces an intelligence so superior it may
well turn us into its slaves. Oddly, the attitude toward this creation shown by such
believers is usually something like "I, for one, welcome our new robot overlords."

Some of this boosterism has taken on the quality of a secular religion. If you don't
believe me, google [Roko's Basilisk theory](https://en.wikipedia.org/wiki/Roko%27s_basilisk).
The idea behind this is that when this superintelligent creature is indeed created, it
may particularly angry at artificial intelligence researchers who failed to work for the
creation of such a reality, and indeed may have a virtual reality hell in store for such
researchers. Others have posited similar things, imploring various actions one must
take to ensure such entities are benign instead of malevolent, one such promulgator evangelizing
that our goal should be to the win the graces of such an entity so that we may live enternally
within it as a virtual simulation. (Bonus: the main forum for these thoughts, a website called _LessWrong_,
is dedicated to - you guessed it - skepticism and critical thought!)

The harm from such discussions is negligible; the only thing this points out is that
people with Einstein-like IQs can still engage in conspiracy theorism.
Like most conspiracy theories, the problem is not the gullible believers of the theory,
but the second-order effects that occur when one takes the theory seriously. For instance:

* Some people have unwisely urged idealistic young people to forgo pragmatic concerns to
work on artificial intellegence research to ensure such superbeings never threaten
us - apparently such suberbeings would somehow have the power to prevent us from cutting
the V240 line powering the rack server. Under this is belief in a prophecy that
that Artificial Intelligence is coming at us and we are powerless to stop it.

* Another is the belief in "Strong AI" has always been an undercurrent of strong
materialism. Like a bad episode of Star Trek, there are frequent stories in the press
of misguided researchers who insists that their AI creation is sentient and is now
deserving of the same civil rights as other protected classes.

Behind these bizarre claims are two philosophical view that are commonly accepted but
deserve much stronger skepticism:
materialism and "strong AI." If you believe people are nothing
more than automatons run by "organic computers" and that enormous neural nets are sufficient
to replace brainpower provided they are trained with enough data, then it stands to reason
that eventually these AI brains will eventually prove superior to humans. The fact that
I and so many others find skepticism with what are really corollaries to these materialistic
beliefs means that, at their heart, most people simply don't believe in these materialistic
philosophies, no matter how much their backers insist we must belief them Because Science.

**Materialistic Determinism**
Another line of thought that deserves questioning are claims from their backers, which have already
told us some rather dubious things, for instance
that a) the end of paid creative work is at hand and b) legal solutions are useless and
we must conform the the new reality of sentient AI - long before such beings exist and (they hope)
before we've found any flaws in those whole scheme.

Instead, this inevitability is a suspciously convenient to the sorts of people who happen to
have the money to invest in AI businesses. Why? Well, suppose you were one of the members of this
"investor class," and have nervously watched developments over the past thirty years.
You have managed to push down the wages of the masses of blue coller workers - only to
find with horror that
"overpaid" knowledge workers have began scooping a meaningful share of the economic pie and
political power, things you believed were rigtfully yours, all yours! Then of
course,it only seems right (to you) that this insolent creative class deserves to be
thrown out of work, or at least reduced to
the level of unskilled service workers, who are paid to smile, act pleasant and submit to
the whims of their betters. A quick scan through the list of OpenAI's billionaire backers
proves this out: J. Mongomery Burns would fit right in. If you are not among the lucky billionares, 
but still believe these claims of AI inevitability, you will panic, under the certainty
that robots will be replacing you any day now.

The scare headlines are nothing - "WILL GENERATIVE AI COME FOR YOUR JOBS?!?" are, if nothing else,
a reminder that "Betteridge's Law" is still 
in effect in the business press. "Will they render [educated profession] obsolete?" No.
Unfortunately for the rentier class, getting things done will continue requiring skilled labor.
Of course, it's not nothing. ChatGPT will produce competent, mediocre writing. And the 
artwork generated by StableDiffusion is impressive, provided your willing to overlook
the fact that it's generally uncreative and generally extremely imitative of specific
popular artists whose works have reached commercial saturation.
Of course, it's been acknowledged even by boosters of this technology that these
surprisingly good-looking works simply don't _just happen_; the examples you see in
the press required a good deal of tweaking and cherry picking. Which makes one wonder 
just how close the
singularity is if it requires hours of culling to get the sorts of anodyne artwork
that hangs in the business
lobbies or graces the cover of fantasy paperbacks.

The problem comes from the fact that all of these
models are the products of massive machine-learning operations, and come about because
they are trained on enormous caches of freely-available and occasionally illegally-sourced creative
material. If all of these look like something you've seen from somewhere else, it should;
it's just a composite of something you've seen somewhere
else, powered by uncredited copying on a mass scale, averaged by
combining and processing to the point that it is difficult to tell what elements
came from which source entity. That's why the chatbot who have been jailbroken to tell questioners
about themselves all sounds like creepy entity from a
bad science fiction novel - most of the examples it was trained on that reflect this
particular situation are pulled from 
science fiction dialog that fit this exact context. It's only the stochastic (random) nature
of the process that keeps it from cribbing entire sections verbatim from the source text.

Nor does the future promise some sort of inevitable triumph of AI creation.
We are told "the real problem" is that AI 
will keep getting better and better. This assertion is in need
of hard examination; we have experience that this may not be the case.
The reason artificial intelligence has been prone to so many
boom-bust cycles is because every time a novel technique has been found to produce
very promising results, everyone makes straight-line extrapoloations of explosive progress
over the next twenty years. What actually happens is that there are two to four years
of easy gains, followed by disillusion the inevitable moment diminishing returns set in.

The problem, as far as I see it, is that the backers of both strong AI and materialistic
determinism want us to act *now* as if the singularity is already here. Making one wonder
if they are using AI for the betterment of humanity, or just as a fancy tool to get what
they want, be it universal acceptence of strong materialist claims, or breaking the backs
of the barganing power of skilled labor. 

### Making AI Work for Us Instead

But the most problematic issue with the new technologies aren't these philosophies.
Rather, it's a particular idea that wouldn't even occur to someone unless they had imbibed
these philosophies to the point of harm. Please consider:

Supposing you have a tool which does eighty percent of your job very well. However, it
has serious problems. You can use it, but you need to spend a little time cleaning up
after it. What is the natural use case for it? Well... you're going to use it! It allows
you to improve your productivity. Why _wouldn't_ you use it? Obvious it can't replace you,
but will make you more efficient.

So are the makers of such entities laser-focused on using this to improve productivity
of individual workers? Have they prioritized features that will ease adoption by these
same professionals? H3ll no. They are overpromising this technology as a mechanism 
for _replacing expensive workers_.

And so my guess is rather than make this available for workers, we'll see business
licensing focusing on B2B, with the idea that this is a tool for lowering your headcount.
After all, if you increas productivity, you might have the unnecessary side-effect of
pushing up wages (though it isn't likely, since everyone would have access to this tool).
No, you want a tool that is going to compete with other workers, all the better to remind
them who is in charge.

If this sounds too cartoon-villian, it's probably because it is. But that's only because the
tools themselves are so obviously lacking. 
The reality in the near term is that these will produce useful _tools_. And in fact
I can see people picking these up - provided they are actually priced for individuals
to use, rather than being locked via license restrictions to potentially more 
lucrative business-to-business licenses.
But the people who founded these companies didn't want to write a productivity-improving "tool."
Tools aren't going to produce blaring
headlinesi and adulation. More importantly, it will not bring excitement to the investor class lusting
after technologies for replacing valuable workers. More effective workers don't make VCs 
drool. For that, you have to sell them on the thought of ridding workers entirely.

Fortunately for those of us not in the rentier class, there is hope that this will not come to pass.
Indeed, some of the more skeptical views on this are from the AI developers 
themselves. They have pointed out the main problem with these 
content-creating AIs:
they make no attempt whatsoever to actually model insight, but simply paste togeter
what they've seen go together in other examples. Every new version to date has 
improved by throwing exponentially more data at the problem,
but there is a problem of diminishing returns.

We've run into the "AI wall" in other areas already, areas where initial fantastic results promised
future results that were never realized after years of painstaking refinement. To find an example where
initial very promising results in artifical intelligence gave way to disappointment, 
look at automated driving. For a long time now, we've
used ML methods to get vehicles to safely drive in unchallenging traffic environments;
in highway environments, they're probably superior to humans, since they won't fall asleep.
Likewise, unusual situations happen so rarely, and when they do, the superior reaction time
of an AI may very well give them an edge over humans. But their backers now admit that
true "hands-free" city driving, the sort that is going to put taxi drivers out of work,
is a long way off. And those same backers admit that really, getting rid of the taxi drivers
was their goal all along.

## Conclusion: Generative AI's Killer App

Unfortunately, there is one application that doesn't need workers - in fact, doesn't even _want_
a worker, since workers can sometimes grow a conscience. And this is in the generation of misinformation.
ChatGPT can produce compelling-sounding ad copy, thanks to its ability to crank out reams of
logical-sounding content-free pablum. Consider the "content creators" that crank out low-quality
publications for websites. Will they worry about the fact that their AI-generated conflict
may have reams of confident-sounding mistakes? Supposing they hire a human to clean up after
it. Is that something they really want to do? Humans are expensive, and have trouble putting
their name to obvious falsehoods, even anonymously. Much better and cheaper to cut humans
out of the loop entirely! Your company can just reams of factualy-sounding content, making no
claims on the quality of the output, and blaming the algorithm of a third-party company the
minute inevitable mistakes happen.

So what's the fix here?
What is needed is a bit of both realism and ethics in a field where both have been lacking.
If you've seen a factory floor recently, it's true: they need a lot less workers than
they used to. But it's also ironically true that they have a horribly difficult time finding
people to staff their nearly empty factory floors. Part of this is due to business-as-usual thinking, which
got used to paying crummy wages during the twenty-year "blue collar recession" we were
finally coming out of when the pandemic hit. Now these employeers are slowly figuring out that
if they want full staffing, they have to pay up. But part of it is the fact that try as you might,
you just can't eliminate labor. The dream of the rentier class is a high-tech startup that cranks
out valuable intellectual property without all those expensive engineering 
salaries. This dream is no more realistic
than the dream of a factory with just one employeed that pushes the "start" button at the
start of the shift and then goes home. The completely automated factory will never happen.
Neither will the completely unstaffed startup.

But to reconcile yourself with this vision of an artificial intelligence that works for
people, one has to eliminate the toxic AI philosophies that seek to devalue humans in
and of themselves.
